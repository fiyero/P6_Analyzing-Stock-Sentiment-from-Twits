{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 6: Analyzing Stock Sentiment from Twits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When deciding the value of a company, it's important to follow the news. For example, a product recall or natural disaster in a company's product chain. You want to be able to turn this information into a signal. Currently, the best tool for the job is a Neural Network. \n",
    "\n",
    "For this project, you'll use posts from the social media site [StockTwits](https://en.wikipedia.org/wiki/StockTwits). The community on StockTwits is full of investors, traders, and entrepreneurs. Each message posted is called a Twit. This is similar to Twitter's version of a post, called a Tweet. You'll build a model around these twits that generate a sentiment score.\n",
    "\n",
    "We've collected a bunch of twits, then hand labeled the sentiment of each. To capture the degree of sentiment, we'll use a five-point scale: very negative, negative, neutral, positive, very positive. Each twit is labeled -2 to 2 in steps of 1, from very negative to very positive respectively. You'll build a sentiment analysis model that will learn to assign sentiment to twits on its own, using this labeled data.\n",
    "\n",
    "The first thing we should to do, is load the data.\n",
    "\n",
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral.\n",
    "\n",
    "\n",
    "To see what the data look like by printing the first 10 twits from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twits.json') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "twits['data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length of Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twits['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiments[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "\n",
    "    text = message.lower()\n",
    "    \n",
    "    text = re.sub(r'https?://[^\\s]+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\$[a-zA-Z0-9]*', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'@[a-zA-Z0-9]*', ' ', text)\n",
    "\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w) for w in tokens if len(w)>1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits \n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('P6_tokenized.p','rb')as f:\n",
    "    tokenized=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter away empty tokenized twit and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tokens before removing empty ones: {}\".format(len(tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tokens = [idx for idx, token in enumerate(tokenized) if len(token) > 0]\n",
    "tokenized = [tokenized[idx] for idx in good_tokens]\n",
    "sentiments = [sentiments[idx] for idx in good_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tokens after removing empty ones: {}\".format(len(good_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = [word for twit in tokenized for word in twit]\n",
    "st\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "we want to create a vocabulary and count up how often each word appears in our entire corpus. Use the [`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter) function to count up all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_tokens = [word for twit in tokenized for word in twit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bow = Counter(stacked_tokens)\n",
    "\n",
    "bow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter away too frequent/rare  Words in Message\n",
    "- remove some of the most common words such as 'the', 'and', 'it', etc. These words don't contribute to identifying sentiment and are really common, resulting in a lot of noise in our input. \n",
    "- If we can filter these out, then our network should have an easier time learning.\n",
    "\n",
    "- We also want to remove really rare words that show up in a only a few twits. \n",
    "- Here you'll want to divide the count of each word by the number of messages. Then remove words that only appear in some small fraction of the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since already remove stopwords, may not need to remove most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_words = len(stacked_tokens)\n",
    "\n",
    "\n",
    "freqs = {key: value/total_num_words for key, value in bow.items()}\n",
    "\n",
    "\n",
    "low_cutoff = 5e-6\n",
    "\n",
    "high_cutoff = 15\n",
    "\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_most_common = [word[0] for word in bow.most_common(high_cutoff)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.most_common(high_cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word[0] for word in bow.most_common(high_cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(K_most_common)\n",
    "print(len(filtered_words))\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Vocabulary by Removing Filtered Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: i for i, word in enumerate(filtered_words,1)}\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vocab = {i: word for i, word in enumerate(filtered_words)}\n",
    "id2vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [ [w for w in twit if w in vocab] for twit in tokenized ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the classes\n",
    "- Let's do a few last pre-processing steps. we find that 50% of the twit are label as neutral. \n",
    "- This means that our network will be 50% accurate just by guessing 0 every single time.\n",
    "- We should balance our classes to help our model learn.\n",
    "- We make sure each of our different sentiment scores show up roughly as frequently in the data.\n",
    "- What can go through each of our examples and randomly drop twits with neutral sentiment. \n",
    "- What should be the probability we drop these twits if we want to get around 20% neutral twits from 50% neutral? We should also take this opportunity to remove messages with length 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "\n",
    "n_neutral\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    \n",
    "    if len(message) == 0:\n",
    "\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "print(n_neutral)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert vocab tokens into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in twit] for twit in balanced['messages']]\n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = balanced['sentiments']\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Implement the text classifier\n",
    "we use softmax instead of sigmoid. The reason we are not using sigmoid is that the output of NN is not a binary. In our network, sentiment scores have 5 possible outcomes. We are looking for an outcome with the highest probability thus softmax is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout     \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)   \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embed_size, \n",
    "                            hidden_size=lstm_size,\n",
    "                            num_layers=lstm_layers, \n",
    "                            batch_first=False, \n",
    "                            dropout=self.dropout)\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=lstm_size, out_features=output_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size):        \n",
    "        weight = next(self.parameters()).data \n",
    " \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        \n",
    "        x=x.type(torch.LongTensor).to(device)\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "\n",
    "        lstm_out = lstm_out[-1, : , :]\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)        \n",
    "        log_probs = self.logsoftmax(out)\n",
    "        \n",
    "        return log_probs, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare x_train, x_valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=0.8\n",
    "valid_split = int(len(token_ids) * split)\n",
    "\n",
    "train_features = token_ids[:valid_split]\n",
    "valid_features = token_ids[valid_split:]\n",
    "train_labels = sentiments[:valid_split]\n",
    "valid_labels = sentiments[valid_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DataLoaders and Batching\n",
    "Now we should build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as batches. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
    "\n",
    "If we set our sequence length to 40, what do we do with messages that are more or less than 40 tokens? For messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(twit_content, labels, sequence_length=30, batch_size=32, shuffle=True):\n",
    "\n",
    "    if shuffle:\n",
    "        indices = list(range(len(twit_content)))\n",
    "        random.shuffle(indices)\n",
    "        twit_content = [twit_content[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(twit_content)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_twit_content = twit_content[ii: ii+batch_size]\n",
    "        \n",
    "        batch = torch.zeros((sequence_length, len(batch_twit_content)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_twit_content):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_twit_content)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw data from first batch and check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=512)))\n",
    "print(text_batch.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)+1\n",
    "embed_size=1024\n",
    "output_size=5\n",
    "lstm_size=512\n",
    "lstm_layers=2\n",
    "dropout=0.1\n",
    "batch_size=512\n",
    "\n",
    "model = TextClassifier(vocab_size, embed_size, lstm_size, output_size, lstm_layers, dropout)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize the weight and hidden state for the first time training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "hidden = model.init_hidden(batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fast pass the the first batch of data into our model to check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logps, hidden = model.forward(text_batch, hidden)\n",
    "logps.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('p6_epoch.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "clip = 5\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "best_val_accuracy=0\n",
    "for epoch in range(epochs):\n",
    "    print(f'Training with epoch {epoch + 1}')\n",
    "    \n",
    "    train_loss = 0\n",
    "    steps = 0    \n",
    "\n",
    "    for text_batch, labels in dataloader(train_features, train_labels, \n",
    "                                         batch_size=batch_size, sequence_length=20, \n",
    "                                         shuffle=True):\n",
    "        hidden = model.init_hidden(batch_size=labels.shape[0])\n",
    "\n",
    "        steps += 1\n",
    "       \n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each=each.to(device)\n",
    "        \n",
    "        model.zero_grad() \n",
    "        \n",
    "        log_probs, hidden = model.forward(text_batch, hidden)        \n",
    "        \n",
    "        loss = criterion(log_probs, labels)\n",
    "        \n",
    "        loss.backward()  \n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()  \n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            for text_batch, labels in dataloader(valid_features, valid_labels,\n",
    "                                                batch_size=batch_size, sequence_length=20,\n",
    "                                                shuffle=True):\n",
    "\n",
    "                valid_hidden = model.init_hidden(labels.shape[0])      \n",
    "                \n",
    "                text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "                for each in valid_hidden:\n",
    "                    each=each.to(device)\n",
    " \n",
    "                valid_log_probs, valid_hidden = model.forward(text_batch, valid_hidden)\n",
    "                valid_loss = criterion(valid_log_probs, labels)\n",
    "                                \n",
    "                probs = torch.exp(valid_log_probs)\n",
    "                top_prob, top_class = probs.topk(1)\n",
    "                equality = top_class == labels.view(*top_class.shape)\n",
    "                valid_accuracy = torch.mean(equality.type(torch.FloatTensor))\n",
    "                                \n",
    "            train_losses.append(loss.item())\n",
    "            valid_losses.append(valid_loss.item())\n",
    "            valid_accuracies.append(valid_accuracy.item())\n",
    "\n",
    "            model.train()\n",
    "            current_val_accuracy=sum(valid_accuracies)/len(valid_accuracies)\n",
    "            \n",
    "            print(f'Epoch: {epoch+1} / {epochs} \\tStep: {steps}',\n",
    "                  f'\\n  Train Loss: {loss.item():.3f}',\n",
    "                  f'  Valid Loss: {valid_loss.item():.3f}',\n",
    "                  f'  Valid Accuracies: {valid_accuracy.item():.3f}')\n",
    "            \n",
    "            if current_val_accuracy > best_val_accuracy:\n",
    "                \n",
    "                torch.save(model,'p6_epochwostop.pth')\n",
    "                best_val_accuracy=current_val_accuracy\n",
    "                print(\"New best accuracy model is saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accy', color='tab:purple')\n",
    "ax2.plot(train_losses, label='train_losses', c='purple')\n",
    "ax2.plot(valid_losses, label='valid_losses', c='red')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "Now we have a trained model, try it on some new twits and see if it works appropriately. Remember  need to preprocess it first before passing it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    tokens = [word for word in tokens if word in filtered_words]\n",
    "    tokens = [vocab[word] for word in tokens] \n",
    "        \n",
    "    text_input = torch.tensor(tokens).unsqueeze(1)\n",
    "    \n",
    "    hidden  = model.init_hidden(text_input.size(1))\n",
    "    \n",
    "    logps, _ = model.forward(text_input, hidden)\n",
    "    pred = torch.exp(logps)\n",
    "    \n",
    "    return pred.to('cpu').detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(device)\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "tokens = preprocess(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word for word in tokens if word in filtered_words]\n",
    "tokens = [vocab[word] for word in tokens] \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = torch.tensor(tokens)\n",
    "text_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = torch.tensor(tokens).unsqueeze(1)\n",
    "text_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "- predict as label 4, with ~71% accuracy\n",
    "- here label4= index3=score 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_twits.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "\n",
    "    for twit in stream:\n",
    "\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
